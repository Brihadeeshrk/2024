---
date: 2024/05/14
layout: article
title: April 2024
description: Notes for the month of April, 2024 (still going on)
tag: langchain, learn, ai
author: You
---

this notes file will be making comparisons with pdf.ai to understand how thats how working

# But first, how does pdf.ai work?

so, there could be one of 2 ways this works

## approach 1

we take the pdf, and the user's prompt and we send EVERYTHING to chatgpt in the hopes that chat will answer it correctly, but more often than not, chat gpt has a word limit and the more content you send, the more money youll end up spending because more words, more money

## approach 2

we take the pdf, extract all the content from inside it, and divide it into chunks of say, 1000 words (which is configurable)
then, store a summary of what each chunk is trying to say
when a user asks some question, find the chunk of text that IS MOST RELEVANT to the users question
and send the relevant chunk and the users question to chat gpt so chat can answer

we'll be going with approach 2 (obvi)
to generate this 'summary' what we do is, we take a chunk and we send it to what is known as an 'EMBEDDING CREATION ALGO' (ECA) and this is created something known as an 'EMBEDDING'

now, this ECA is a key topic throughout this lecture
an EMBEDDING takes a string and turns it into an array of numbers, this array is always going to be 1536 elements long, and each of these values ranges btw -1 and 1
what do these elements mean? theyre discussing or rating the raw essence of what the text is talking about

eg: for the string "Hello World!!"
the embedding may look something like this (this is random but just for examples' sake)
[0.9, 0.3, -0.3, ...... ]
now the first element may be a score of how happy the string is,
the 3rd element may be a score of how much this text is talking about mountains etc...

and since, these are numbers, we can end up doing math operations, which help us a lot
and we're going to do these embeddings for every chunk and get a list of embeddings

and once we get all these chunks, were going to store them in a db, we usually refer to these dbs that are specialised in storing embeddings as VECTOR STORES

now, how do we find a chunk that is MOST RELEVANT to a chunk?
we pass this query ALSO into the ECA and now we have an array of this query also right?
now, we're going to do some math and check if this embedding is similar to the chunks in the vector store

lets say, that chunk #3 is the most relevant one, we take the query and the chunk and send it as a prompt to chat
and the response we get is what we show on the screen

how do we send it to chat gpt?
we basically send the chunk and the query in one prompt, and anyone can basically tell the answer when its right in the same paragraph right? which is why chat gpt is one of the least interesting bits in this whole thing bcoz its not really doing anything spectacular

# why langchain and how does it work?

langchain gives us tools to automate every single step we mentioned in approach 2

goal of langchain
provide interchangeable tools such as automate each step of a text generation pipeline
has tools for loading data, parsing, storing, querying, passing it off to models like gpt
integrates with a ton of diff services provided by a ton of diff companues
relatively easy to swap out providers, dont want to use gpt? swap with a diff model in a few mins

# chains

what are the goals of using langchain?
1 - provide tools to automate every step of a text generation pipeline
2 - make it easy to connect tools together

a **chain** is the most fundamental and most important aspect of langchain, and its a class offered by LC, we use them to create reusable text gen pipelines
we may use multiple chains and we can combine them to create complex pipelines
a chain is composed of 2 ele, a prompt template and a language model

## prompt template

it produces the final prompt thatll be sent to the LLM. it needs to declare all the vars it needs to build this prompt, in the current example where we're going to ask an LLm to write us a program in a particular language, our vars in this case would be language and task

## language model

the llm that we wish to use to get this pipeline to work
gpt, bard, claude etc

## what are the inputs to a chain?

its a dictionary that must contain values for each var that the prompt template requires

## what are the outputs to a chain?

its also a dict that contains the inputs AND the generated content, and this generated content will be in the 'text' key. and this 'text' can be changed to anything, could be renamed to response and so on

## interlinking chains

to feed the output of one chain directly into another chains input, we need to import another package from LC called SequentialChain

# deep dive into interactions with memory management

lets understand some terminology and concepts,
LLM = large language model, its an algo that generates some amount of text, there are MANY models out there
when we make use of LLMs there're 2 styles of interfaces that we get
most LLms follow a COMPLETION style of text gen

note, when dealing with docs from langchain, when the docs say llm, its assumed youre using a traditional model and a lot of the classes are built for trad style

what is completion/ traditional LLM? lets say you give an input saying "im a comedian who jokes about taxes, and you sauy, have you ever noticed how"
now, a traditional LLm would take the first statement into consideration and really complete this statement, very much like a fancy autocomplete

another style is a CONVERSATIONAL model, some llms have been adjusted to have a back and forth type of exchange, so we say something, we get something back
we say something else, we get something else back. but under the hood, these are still 100% completion models but have been tweaked

now for a completion style llm, its fairly easy, weve already done it before
input -> llm -> output

but for a conversational style llm, the interface is more unique. why? because we have to somehow distinguish between my messages and the chatbots responses.
so, when dealing with convo llms, there are 3 entities

1. user message
2. assistant message - message sent by the llm
3. system message - a message to customise and dictate how the chat bot behaves. usually set by developers
   an example:
   for the system message: empty

user message - what is html
assistant message - html stands for .....

for the system message: you are very rude and unhelpful

user message - what is html

assistant message - stfu

so, now we can make a list of all the messages
the system message goes at the top
followed by user
and then assistant
and then user
......

the thing with chat models, is this
now, lets say you ask a follow up question to smthn the assistant said, if you just asked why or what? the assistant would probably reply why what?
so, whenever we deal with chat style responses, we more than typically sent the entire message history every time we want to extend a conversation.

now for a conversational style llm, its fairly easy, weve already done it before

chatpromttemplate - nested templates
systemMessagePrmoptTemplate
HumanMessagePromptTemplate

input -> chatPromptTemplate -> llm -> op

## chat MEMORY

MEMORY is a class offered by LC that used to store data in a chain
MEMORY is made use 2 times in a chain, once when we initially call our chain and send some input vars
and then afer creating the template and getting the response from the chat llm, we call the same exact memory obj again
ONE CHAIN ONE MEMORY

### what does memory do

when we first run the chain, the input vars are sent off to the memory obj and in that point, memory can take in those values, or it can take extra ones
after we get the response from the chat llm, the outputs are sent to the memory and the memory has the chance to inspect and store some of it

### what memory does in a chat chain

LC has many kinds of memories
ConversationTokenBufferMemory, CombinedMemory, ConversationBufferWIndowMemory, ConversationBufferMemory and so on
a lot of these memorues are really in a way, completion based LLMs

but we arent using completion models, so the memory that has support for chat based LLms is ConversationBufferMemory and this is what were using to store the messages
So, this ConversationBufferMemory (CBM) is going to store all the messages that we send and get back
what does CBM do after we get a response from an LLM, it takes our message, the HumanMessage and the output, ie AIMessage and stores in messages

### but what does memory NOT handle?

the memory, once it puts these messages into the input vars, it doesnt actually take them and push them into the model

### how to actually deal with storing these messages?

so inside our chatPromptTemplate we have our messages property right, what we could do is, we could add another field to this array called MessagesPlaceholder(variable_name='messages'), now this messagesplaceholder is going to look at the message property coz thats the config key we've put within the ()
and now, this placeholder is going to replace itself with every message, be it human and/or ai

### now dealing with saving these convos in a file called messages.json so that we can revisit them once we restart our file

we need to import another class from langchain.memory and thats the FileChatMessageHistory and if we change the memory config slightly like this

```python
memory = ConversationBufferMemory(memory_key="messages", return_messages=True, chat_memory=FileChatMessageHistory('messages.json'))
```

and another logical question is, lets assume you have a VERY LONG convo, thats going to make this json file 1000000000+ lines long right? the files going to get too big
there is a limit to that we can send to our LLm model, and if we're paying (in my case, we are) the longer the convo, the more were paying

so, were going to use ConvertionSummaryMemory and its going to replace ConversationBufferMemory
ConvertionSummaryMemory doesnt work very well with FileChatMessageHistory, so we're going to use one or the other

# adding context and embedding techniques

in this section we have a file called facts.txt and we're going to be asking the llm questions based from this txt file
like eg: a fact may say; the color red is the most famous one

and i could ask a question like, which color is famous
so, the goal is find the most probable fact, send it to gpt and have it answer us the prompt

now we need to load the facts.txt, we could do it using std py libraries but lets try and do it using LC
LC provides classes to help load data from different types of files
these are called LOADERS
for .txt - TextLoader
.pdf PyPDFLoader
.json JSONLoader
.md UnstructuredMarkdownLoader

whats interesting is that, LC also gives us the classes to load up any kind of file from different locations, such as S3 bucket, called S3FileLoader
this class will import all the files in this bucket regardless of their type
note, some of these loaders are built on top of other packages, like PyPdf and so on

## what do we mean when we say load a file?

all these laoders, take a file and give back something known as a DOCUMENT. a doc is very imp thing inside of LC
and document is a class in LC
and every doc is going to have ATLEAST 2 properties, 1 - pageContent and 2 - Metadata
metadata could store info like where did we get this data from etc

## search criteria

now, what are some ways we could potentially approach this problem?
1 - we could take the entire doc along w the prompt and send it to gpt
with this, we have MANY downsides, such as: longer prmopts - so more money, take longer to get an answer
2- another approach is, we could count the no: of words in the prompt, and find occurences of these words in the facts, but this way what if we use different words in the prompt but are trying to ask a very basic fact? this wouldnt work then right?

which is why we need to explore embeddings

## introducing embeddings and semantic search

an embedding is a list of numbers btw -1 and 1 that score how much a piece of text is talking about some particular quality
we refer to the number of elements in the array as dimensions, for eg: for a sentence: amy likes to jump over rocks bravely
for dimensions BRAVERY and HAPPINESS, the embedding would be [1,1]
as, the sentence talks about her happily and bravely jumping

now we can plot these points on a 2d graph since we have 2 dimensions
and from the origin (center) we can draw arrows to these points, also called as VECTORS
we can come up with a way to deduce how similar these lines are
1 - what is the dist between these 2 points, and we could repeat this process among all the lines and we can then deduce that the 2 points that have the shortest distance, are the most simiilar. called as the SQUARED L2 METHOD
2 - look at the angle btw 2 lines also called COSINE SIMILARITY - using the angle btw 2 vectors to figure out how similar they are

## embedding flow

divide the file into chunks
calculate the embedding for all the chunks
store embeddings in a specialised db for embeddings (vector store)
take the users question
embed the question
do a similarity check with our stored embeddings to find the ones that are most similar to the users question
put the most relevant 1-3 facts into the prompt along with the users question

## chunking

```python
from dotenv import load_dotenv
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

load_dotenv()

text_splitter = CharacterTextSplitter(
								separator="\n",
								chunk_size=200,
								chunk_overlap=0
				)
# chunking
loader = TextLoader('facts.txt')
docs = loader.load_and_split(
	text_splitter=text_splitter
)
```

we use the CharacterTextSplitter from langchain, and this is an explanation of how we'er chunking
so, we first count he numbers of chars using the chunk_size var, and once we reach 200 chars, we look for the nearest separator, once we find that, then that becomes our first chunk. and chunk overlap is to put some kind of copying of text between each individual chunk

lets say we give a very unealistic chunk size of 10, then langchain would find the nearest separator and make the content till that separator a chunk, and since this violates the chunk_size property, langchain throws us a warning

## generating embeddings

there are many ways and many algos that we can use to create these embeddings, but in this case were only going to talk about 2 embedding models
1 - SentenceTransformer - uses a set of algos that are going to run on your comp to calculate embeddings. will create 768 dims
2 - OpenAI Embeddings - will create dims of 1536 dims. will cost money

does this mean openai embeds are better? depends on use case
but, we cant compare the 2 embeds with one another, as they are not compatible

we're not going to sit and un the embeddings as they cost money, but we're going to wait until we have a place to store these embeddings so we can save money

# custom doc retrievers

## chroma db

its a vector store that runs locally, internally uses sqlite

```bash
	pip install chromadb
```

```python
# what youre doing here is, youre creating a new instance of Chroma and the from_documents

# states that, you want to immediately calculate the embeddings for all the documents in docs

# and after it runs, its going to stored in a sql directory called emb

db = Chroma.from_documents(docs, embedding=embeddings, persist_directory='emb')



results = db.similarity_search_with_score("What is an interesting fact about the english language?") # this will give us the similar records WITH score



# if we run this line without k, then you may notice that we get MANY duplicates, so, when we run the Chroma.from_documents line we're essentially trying to calculate

# the embeddings for them and we're going to store it in the db, and that means everytime we run it, we get a lot of duplicates and we dont want to do it



# so we need to build smthn, that finds these duplicates and gets rid of them

results2 = db.similarity_search("What is an interesting fact about the english language?") # this will give us the similar records WITHOUT score




# for result in results:

# print('\n')

# print(result[1])

# print(result[0].page_content)



for result in results2:

print('\n')

print(result.page_content)
```

## building a retrieval chain

so what happens is that, when we ran our script for the first time, it worked properly
but as we kept running, more and more embeddings were created and stored in the vector store, which is now causing the same chunk to be delivered multiple times (duplicates) because its stored so many times

so in order to counter this, we're going to separate our logic into 2 files, one file to load the txt file, parse and load it into chroma
and another file thats going to run the Q&A process

1. `chat = ChatOpenAI()`: Initializes a ChatOpenAI instance for conversational AI.
2. `embeddings = OpenAIEmbeddings()`: Initializes OpenAI embeddings for text processing.
3. `db = Chroma(persist_directory='emb', embedding_function=embeddings)`: Initializes a Chroma instance for vector storage and retrieval. Parameters:

   - `persist_directory`: Directory to persist Chroma data.
   - `embedding_function`: Function for generating embeddings (in this case, OpenAIEmbeddings).

4. `retriever = db.as_retriever()`: Creates a retriever object from the Chroma instance.
5. `chain = RetrievalQA.from_chain_type(llm=chat, chain_type="stuff", retriever=retriever)`: Initializes a RetrievalQA chain for question answering using a given language model and retriever. Parameters:

   - `llm`: Language model for generating responses (in this case, ChatOpenAI).
   - `chain_type`: Type of retrieval chain to use (e.g., "stuff" for general purpose).
   - `retriever`: Retriever object used to retrieve relevant documents.

6. `result = chain.run("What is an interesting fact about the english language?")`: Runs the retrieval chain with a question/query.
7. `print(result)`: Prints the result obtained from the retrieval chain.

### what is a retriever

in the world of LC, a retriever is an obj that can take in a string and return some relevant docs
to be a retriever, the obj must have a method called "get_relevant_documents" that takes a string, and returns a list of docs

now, in the main.py file, if you remember we ran a query called similarity_search, now this is a query that is specific to chroma
it does the same thing (almost) as a retriever but its specific to chroma

now, LC is a framework that enables us to mix and match dbs, engines etc right? so the devs of LC made it such that, the developers of the DBs themselves have to expose a function called get_relevant_documetnts if you want to work with RetrievalQA
this way, LC doesnt dive into the specifics of DB, but handles it from like an abstract view

which is why, here we create a retriever from chroma

```python
retriever = db.as_retriever()
```

### what is chain_type="stuff"

so when we say chain_type="stuff"
what does stuff mean?
so once we run and find chunks that match our query, what are we doing? we're adding it to the system message and asking gpt to ans our query right?
in other words what are we doing? we're STUFFING all the relevant chunks into 1 query right? thats why it says stuff
most basic form

3more chain types exist
map_reduce
map_rerank
refine

way more complicated than stuff

#### map_reduce

takes significantly longer to run than stuff, thats because we're calling our model 5 times as opposed to just once with "stuff"
so lets go over what this does in the background

we still have our prompt
we still have our vector store
we still have our relevant chunks (fyi by default when we use chroma we get 4 relevant docs)

so for each of these chunks, we're going to feed them into their own SystemMessagePromptTemplate and HumanMessagePromptTemplate, like this
**SystemMessagePromptTemplate**: use the following portion of a long document to see if any of the text is relevant to ans the question, return any relevant text verbatim {chunk}
**HumanMessagePromptTemplate**: here is the users question {prompt}

and each of these chunks is going to give us a response

IMPORTANT
now, lets say the 4th chunk returned by the VS doesnt have any relevant fact, and we send it to GPT, GPT will hallucinate and make up some random fact that isnt relevant from our facts.txt

so once we have the response from all these 4 prompts, theyre all assembled into 1 summary and are sent off to gpt again (5th TIME)
**SystemMessagePromptTemplate**: use the following context to answer the users question {summary}
**HumanMessagePromptTemplate**: here is the users question {prompt}

and then we get a final answer

#### map_rerank

similar to map_reduce but one key difference, we also get a score/rating of how relevant it thinks the response it gives is
so,
we still have our prompt
we still have our vector store
we still have our relevant chunks (fyi by default when we use chroma we get 4 relevant docs)

so for each of these chunks, we're going to feed them into their own HumanMessagePromptTemplates, like this
**HumanMessagePromptTemplate**: use the foll pieces of context to ans the q at the end. if you dont know the ans, just say you dont know, dont make up an ans. in addition to giving an ans, also return a score of how fully it answered the users question {chunk}. it should be in the foll format "....." and also talks about how to rate it eg: 100 if it answers properly, 0 if it doesnt etc
**HumanMessagePromptTemplate**: here is the users question {prompt}

and each of these chunks is going to give us a response

IMPORTANT
now, lets say the 4th chunk returned by the VS doesnt have any relevant fact, and we send it to GPT, GPT will hallucinate and make up some random fact that isnt relevant from our facts.txt
now, since this made up fact is technically relevant to the users question, it will give it a high rating, or it might put something from the doc and give it a low rating or 0
SLIGHTLY better than map_reduce because of the score

and then it finds the highest score and returns that to the user
so, one less gpt call
so 4 API CALLS

#### refine

so,
we still have our prompt
we still have our vector store
we still have our relevant chunks (fyi by default when we use chroma we get 4 relevant docs)

NOW its important to note that _map_rerank_ and _map_reduce_ ran those 4 chains SIMULTANEOUSLY

BUT REFINE is running these chains in SERIES, ONE AT A TIME

so for each of these chunks, we're going to feed them into their own HumanMessagePromptTemplates, like this
**HumanMessagePromptTemplate**: use the foll ctx to ans the user's q {ctx}
**HumanMessagePromptTemplate**: here is the users question {prompt}

gives us a response

now after we get the res from this chain, its taken and is fed into an other chain
**HumanMessagePromptTemplate**: here is the users question {prompt}
**AIMessagePromptTemplate**: << PREV RESPONSE >>
**HumanMessagePromptTemplate**: we have a chance to refine the ans using this additional context {2nd chunk}

and the same goes on for all the 4 chunks /chains
and whatever is returned at the last chunk is returned to the user as the final res

---
date: 2024/07/21
layout: article
title: July 2024
description: Notes for the month of July, 2024 (ongoing)
tag: microservices, docker
author: You
---

continuing last months article here, and in the MANY coming months lol

# running services using docker

## current deployment scenario

- we have servers running on 4000, 4001, 4002, 4003 and 4005
- now a simple solution is to take all this code and place it in a VM and run it there
- lets say comments service is being called too much, so we could create 2 more instances of comments and use a load balancer (load balancer basically randomises which comments server to send it to)
- and then wed have to use 2 more extra ports for these 2 new comments services
- and because we added these 2 new ports, we have to make changes in multiple projects, such as event bus and more
- and this way we're tightly coupling our code to the number of instances of comments
- and if you say we're going to have a separate VM for extra comments, we're still going to have to change multiple files and make changes
- hence we're going to be introd to docker and kube

## why docker

- we're going to create containers
- containers are like isolated computing envs
- it contains everything we need to run a single program
- so we're going to create separate containers for separate services
- if we need extra instances, we can just create another container with comments
- what do we need to run our services? npm and node right?
- now, if we need to run it somewhere we're assuming node and npm are installed no? and thats a big assumption
- and also it requires knowledge on which script to use to run the services
- so docker solves both these problems, its going to contain everything we need to run the service and will also know how to start it etc
- super easy when it comes to running services, not just for node js but for anything

## why kube

- tool for running a bunch of containers together
- when we run kube, we're supposed to give it a config file which tells it which all containers we'd like to run
- and then kube is going to handle comm and nw reqs btw all these containers
- so kube creates this thing called a cluster
- a cluster is a set of diff VMs, each VM is referred to as a node
- theyre all managed by something called a master and this master is a program thats going to manage everything inside of our cluster, all the programs and other aspects
- so lets say we have 3 nodes
  - 2 post
  - and 1 event bus
  - now when an event occurs, we'd have to tell the event bus how to reach the 2 post containers
  - when in fact, kube offers this "channel" , that lets us pass things to it and this channel will forward it to posts
  - so this way communication is super simple
- kube also makes copying containers and scaling super easy

## docker

- why should we use docker?
  - at some point we mustve installed some sw on our laptop
  - during the installation wizard, we may come across some error, we may have looked up the error and fixed that issue
  - and when the wizard continues, we may come across another error and then its the troubleshooting phase all over again
  - so, docker wants to make it super easy & straight forward to install and run sw on ANY device, laptop, servers etc
  - docker makes it really easy to install and run sw without worrying about setup or dependencies
- what is docker?
  - a bit more challenging to answer
  - when you read an article, or talk to someone, when they say "Oh i use docker", they most probably mean that they use the Docker ecosystem, and that Ecosystem consists of Docker Client, Server, Machine, Images, Hub, Compose
  - All these tools are pieces of sw that come together to form a platform to create containers
  - In essence, docker is a platform or ecosystem around creating and running containers
  - so to run redis, we ran `docker run -it redis` so when we ran this, something called docker cli reached out to docker hub and it downloaded a single file called an `Image`
  - an `Image` is a single file that contains all the dependencies and all the config req to run a ver specific program, for eg redis
  - a container is an essence of an image
  - a container is a program with its own isolated set of hw resources, its own memory, its own nw tech, its own hard drive
- installing docker
  - when we install docker, we're installing 2 key pieces of sw
  - called docker client (cli) and docker server (daemon)
  - cli - the tool that were issuing commands to
  - daemon - the tools thats responsible for creating images, running containers etc
- docker bts
  - we installed docker and ran `docker run hello-world`
  - here's all the things that happened bts
    - you gave the command to the cli
    - cli relayed this to the daemon
    - daemon checked if this image is available locally by looking into the `image cache`
    - since we just installed docker, we obv didnt have it, so the daemon reached out to docker hub, which is a repo of free images that we can download and run
    - so the daemon downloaded it from the hub and saved it locally in the image cache, where it can obtained from next time
    - then the daemon took that image, loaded it into the memory and created a container and ran it
- what is a container?
  - but before that,
    - what is NAMESPACING?
      - it is basically isolating your hard drive or any "resource" for that matter for processes or a group of processes
    - what are control groups? (cgroups)
      - they limit the amount of resources used per process
      - amount of memory, cpu, hard drive, and nw bandwidth
  - so in other words, a container is basically just a process and its necessary req resources
  - so the process talks to the kernel which in return talks to the resources that have been available for this process
  - what is the image -> container relation?
    - whenever we talk about images? we're talking specifically about file system (FS) snapshots and a start up command
    - what is a fs snapshot? its a copy-paste of some directories from the FS
    - so what happens when we run this image?
      - the kernel is going to allocate some part of the hard drive for the fs snapshot inside the image and store the snapshot data there
      - and then the startup command is run, and then the process is started and is isolated to just that container
- how is docker running on your device?
  - namespacing and cgroups are specific to linux
  - when we installed docker, we basically installed and are running a linux VM
  - inside this VM, we're creating all these containers
  - and the VM talks to the kernel and allocates resources etc

### creating and running an image

- `docker run <imagename>`
- we can also override the start up command by doing this
  - `docker run <imagename> command!`
  - after the imagename, we supply an alt command to be executed inside the container after it starts up
  - this is an override and the command that was there alongside the fs snapshot will not be run anymore
  - `docker run busybox echo hi there`
    - what does this mean?
    - we installed an image called busybox which contains .exe files called echo, ls, etc
    - and who knows what the startup command for busybox is? and we dont need to know it too as long as we know what command to run
    - so in this example, the busybox image has an exe called echo which repeats whatever you say to the docker cli
    - so when we run this command, we see hi there
  - `docker run busybox ls`
    - same understanding as above
    - busybox has an ls exe
    - so when we run it, we see the files that were copied to the container's hard drive by the fs snapshot
    - so whatever files you see, are all files that were copied from the fs snapshot and ARE NOT YOUR LOCAL FILES

## listing running containers

- `docker ps`
- this command will list all the diff running containers that are currently on our machine
- `docker ps --all`
- this command shows all the commands that have been called by you

## container lifecycle

- running `docker run` is equivalent to running `docker create` + `docker start`
- `docker create <image-name>` - creates a container
- `docker start <image-name>` - starts a container
- what happens when we create a container?
  - the fs snapshot from the image is taken and setup in the container's hardware
- what happens when we start a container?
  - we run the startup command that comes with the image
- now lets create and start a container

```bash
docker create hello-world
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768

docker start -a 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> output

%% what happens if we dont give the -a %%
docker start 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768

%% the -a attribute tells docker to watch for the output %%
```

## utils

### to clear your docker containers

- you can clear your stopped containers, build cache etc by running this command

```bash
docker system prune
```

- so now, if you want to run any image, it would first download the image from docker hub and then run it for us

### to get output logs

- now there maybe times when we forget to add the `-a` flag while using `docker start`
- and if it takes minutes/ hours to run, having to re run it again with the -a flag is just painful
- which is why we can use logging, such that whatever event is emitted from this container, its logged

```bash
docker start 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768

docker log 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> hi there

```

### how to stop containers

- when we used to run docker run, we could stop the execution using cmd+c
- but if we use docker start and docker logs, how do we stop containers?
- `docker stop container-id` or `docker kill container-id`
- stop
  - when we use the stop command, bts, docker sends a signal to the container called SIGTERM
  - aka terminate signal
  - what this does is, it tells the container to terminate the signal in its own time and also gives the container a little time to perform some clean ups
  - if the container doesnt stop within 10s, docker automatically issues the kill command
- kill
  - when we use the kill command, bts, we send the SIGKILL command
  - aka kill signal
  - terminate and dont do anything else
- ideally wed like to stop containers

### execute an additional command in a container

- when using redis
- we usually run 2 commands, `redis-server` and `redis-cli`
- but if we run redis in a container, we cant access this redis server from outside (obvi)
- so then, in essence, we need to have another startup command along with the one that comes already with the image
- but how do we call it?
  `docker exec -it container-id command`
- exec - run another command
  -it - allwos us to provide input to the container
  command - the extra command you want to run
- an example:

```bash
%% shell 1 %%
docker run redis

%% shell 2 %%
docker ps
> CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS      NAMES
> d29c63477078   redis     "docker-entrypoint.s…"   5 seconds ago   Up 4 seconds   6379/tcp   trusting_poitras

docker exec -it d29c63477078 redis-cli
> 127.0.0.1:6379:> set mynumber 5
> 127.0.0.1:6379:> get mynumber
> "5"
```

- what happens if we dont give -it?
  - redis cli will be started but we wont have the ability to provide any inputs to this

#### purpose of -it flag

- in reality, the -it flag is a combination of a `-i` and a `-t` flag
- processes in linux have 3 channels lets say, STDIN, STDOUT and STDERR
- whatever ip you give, goes into the container using the STDIN channel, whatever the container spits out is shown to you via the STDOUT channel, and if any errors occur, theyre shown to you via the STDERR channel
- so when we type `-i` we're saying we want to attach this terminal session to the STDIN channel of the newly running process
- the `-t` flag simply formats it nicely for us to use, it does quite a bit under the hood, but simply for us to understand, it just makes the ips and ops pretty
- also provides autocomplete etc..

### how to get access to the terminal of your container?

- we will want to run commands inside our container without constantly wanting to type exec exec etc
- so if we want to access the terminal of a container, we can run this

```bash
docker exec -it container-id sh
> #
```

- and now we can run any linux or terminal commands like cd, ls, export etc
- and once youre in your shell and if you cant exit using ctrl+c, you can try ctrl+d

#### starting with a shell

- we can also run the `docker run -it busybox sh` command to start the busybox container, but just open the shell
- this way, we can poke around ourselves and execute whatever command we want

## creating docker images

- till now we've used images created by other devs, redis-cli, busybox, hello-world
- the process to create our own image is relatively straight forward, we just have to remember the syntax

1. create a dockerfile
   - this is a plain txt file with a few lines of config to define how our container must behave, what prgms it must contain and what it should do when it starts up
2. send it to docker client
3. client will send it to docker server
   - server is doing all the heavy lifting for us
   - its going to look inside the dockerfile and create an image that we can use
4. we have a usable image

- what is the flow to create a docker file?
  - specify a base image -> run some commands to install additional prgms -> specify a command to run on container setup
- here is a simple Dockerfile

```Dockerfile
# using the flow established earlier
# use an existing docker image as a base
FROM alpine

# download and install a dependency
RUN apk add --update redis

# tell the image what to do when it starts as a container
CMD ["redis-server"]
```

- then we cd into the file where we have this Dockerfile
- and we simply run `docker build .`
- and then in the end, we'll get one id of the image we created, then we can simply do `docker run image-id`

### breaking down the Dockerfile

- the Dockerfile contains a particular syntax
- the first word is an 'instruction'
- and whatever comes after it is an 'argument'
- so the instruction tells docker-server what to do
- the 'FROM' is used to specify what image we want to use as a base
- the 'RUN' instruction is used to execute some command while we are preparing our custom image
- the 'CMD' instruction is used to specify what should be executed when our image is used to start up a container
- FRO RUN and CMD are some of THE MOST IMP instructions, but there are many more that need to be known

#### what is a base image?

- writing a Dockerfile is similar to installing some browser on a comp with no OS
- what are the steps we'd do?
  - install an os -> open default browser -> download chrome dmg -> run dmg right?
- so what we did with `FROM alpine` was very similar
- that was like saying, install this OS (not exactly but just to paint a picture of what its like)
- otherwise it would be an empty image, no infra, no programs that we could use, nothing to help us install extra dependencies, nothing
- so having a base image is to give us a starting point of sorts that we can customise etc
- what is alpine?
  - simple, it contained the necessary programs and functions that we needed to create our custom image
  - very much like asking, why you chose win/ mac as your preferred os- because they provided you with what YOU needed
  -

### breaking down the build process

- so first things first, we check if our build cache if we've installed an image for alpine
- if we haven't, we install that from docker server and create an image with it
- now this image contains 2 parts right? its FS snapshot and some startup running command
- - - The End of Step 1 - -
- Then we come across the RUN instruction which asks the server to install redis right, so what happens here?
- we create a temporary container with that image, using the command we gave as an argument to that instruction as the startup command
- now our FS contains all the files from alpine and also the files for redis server right?
- a copy of the fs is taken and this container is terminated
- - - The End of Step 2 - -
- the 3rd line, is an instruction telling what command we'd like to have as our startup command right?
- so we again spin up a temporary image, and copy the fs snapshot from the previous step and replace the startup command with what is specified in the Dockerfile
- and this final image is the image that we return to the user
- - - The End of Step 3 - -

### rebuilds with cache

- if we re run the build command for this Dockerfile, we can see that it simply says `CACHED [2/2] RUN apk add --update redis `
- in other words, it caches every line of operation of our dockerfile
- lets say we add another line after `RUN apk add --update redis` that says `RUN apk add --update gcc`
- now that our Dockerfile has changed, we dont run every line, the docker server sees which line has changed, and then it only executes the lines that have changed and the lines below that
- and it uses cache to build the lines before it, because they havent changed since the last build
- now, lets say we invert the redis and the gcc line
- now, the server would have to reinstall gcc and redis because now the order has changed and now redis being installed AFTER the changed gcc line
- so, when modifying dockerfiles, its important to make sure we add new liens towards the bottom to maximise cache usage

### tagging an image

- now at the end of the build process, to run the image we just created we have to run attach the long string (smthn like `sha256:950fc54d9b019d2b2e06fae0e3192f65353a081504811372f41e0a989aab71b0`) at the end of docker run right?
- now copying this long string is not difficult, but it would be easier if we could 'tag' it, or in other words give it an alias right?
- for that, we need to modify the build command slightly
  `docker build -t your-docker-id/imagae-name:version .`
- your docker id is what you setup as your username
- you can choose whatever name youd like to set for the image, maybe redis-server or just redis
- and then the version is usually a number, but you could also just put `:latest`
- so your build command should look like this in case you decide to tag your image

```bash
docker build -t briha2101/redis-server:latest .
> ...
> ...
> ...
> Tagged as ...

docker run briha2101/redis-server
```

- so now, when i want to run this image, i can use the tag i just gave
- i can optionally emit the version, because by default the latest version would be selected for container creation
- but technically speaking, the version we specify at the end is the tag, everything else is more like the repo or the project name

### manual image creation using docker commits

- very rare that we would actually do this
- ok, but how would we do this?

```bash
docker run -it alpine sh
> #> apk add --update redis-server
> ...
> ...
> ...
```

- in another terminal shell,
- for this we use the `commit` property along with the `-c` flag and within single quotes, we mention the command we want to set as the initial command and then we follow it with the id of the container

```bash
docker ps
> CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
> efeee12       alpine      "sh"       ...       ...       ...       ...

docker commit -c 'CMD ["redis-server"]' efeee12
> sha256:2473rshfsgsh


docker run 2473rshfsgsh
> ...
> ...
> redis-server initialised
```

### run a nodejs server inside the container and access it from outside

- what does alpine mean?
- in the docker ecosystem, alpine basically means only the most basic version
- so node:alpine would mean, only the most essential and stripped down image of node
- flow - create a nodejs webapp
  ```js
  const express = require("express");

const app = express();
app.get("/", (req, res, next) => {
res.send("Hello World!");
});
app.listen(8080, () => {
console.log("Listening on port 8080 🚀");
});
```

    - create a dockerfile
    ```Dockerfile
    # specify a base image
    # downloadinfg and running an image that has node preinstalled

FROM node:14-alpine

# copy the contents of the current work-dir and paste it into the container

COPY ./ ./

# download and install the dependancies

RUN npm install

# default command

CMD ["npm", "start"]
```	- build image using dockerfile
		-`docker build -t briha2101/simpleweb .`		- if we dont add`:latest`also its ok, its appended by def
	- run image as a container
		-`docker run briha2101/simpleweb`	- connect to webapp from a browser
		- port forwarding
			- when we start the image, the server is running on 8080
			- but if access that on our machine, we cant bevause the container is an isolated env and the traffic isnt routed to the container's ports
			- the container has its isolated set of ports, through which it can receive traffic but by default traffic from our local machine wont be sent into it
			- if we want traffic from local nw to be sent to our container, we need to setup an *explicit port mapping*
			- **port mapping** is essentially saying, if anyone makes a req to port 8080 on your local machine, forward that to one of the container's ports
			- now, our container has no limitation on sending data out, as we've noticed with npm install. the only problem is data in ->
			- how do we enable this? its not something to change inside the Dockerfile, but something we add while running the image
			- the syntax:`docker run -p 8080:8080 briha2101/simpleweb`			- the`-p` flag is for port mapping - then we mention the port on localhost that we'd like to fwd to the container - then we mention the port inside the container - and then the id of the image

```bash
docker run -p 8080:8080 briha2101/simpleweb
```

- note, for forwarding the local nw port and the container dont have to be the same (which is what we'll be doing in prod projects)

### setting up a working dir

- now in the dockerfile when i copied everything from the folder into the container, it placed everything in the root folder
- and this can sometimes cause conflicts when we have folders with the same names, and these folders override existing folders and disrupt services in our container
- which is why we can setup a working directory and mention where in the container we want to store all our files

```bash
WORKDIR /usr/app
COPY ./ ./

RUN npm install
```

- this states that, well be setting the files' location to usr/app
- and then copy the files there
- and then the rest of the processes
- if this folder doesnt exist, it will be created for us

### dealing with changes and rebuilds

- lets make a change in the `index.js` file and see if it reflects on the browser
- OBV NOT, because we havent copied the latest version of the code to the container
- so, how would we do this? we have to build the container again
- so when we do it, docker notices that the files have changed, so it re runs the COPY instruction and all the commands under it
- and that means `npm install` also, and re running npm i when we dont have any new dependancies is troubling when we have many dependancies in our project. so how do we fix this?
- so the goal is to minimise npm i runs right? what does npm i need? a package.json file. so kets just copy that ONE file first

```bash
COPY ./package.json ./

RUN npm install
```

- this way, npm i will only run if theres a change in the package.json file, which is exactly what we want

```bash
COPY ./package.json ./
RUN npm install
COPY ./ ./
```

- and then we can simply do this, where we copy all the remaining files later
- so now, even if we make some changes, we arent running npm i, itll simply use build cache, well only copy the files and the run command

#### notes

- Buildkit will hide away much of its progress which is something the legacy builder did not do. We will be discussing some messages and errors later in Section 4 that will be hidden by default. To see this output, you will want to pass the progress flag to the build command: `docker build --progress=plain .`
- Additionally, you can pass the no-cache flag to disable any caching: `docker build --no-cache --progress=plain .`

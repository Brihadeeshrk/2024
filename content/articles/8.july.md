---
date: 2024/07/21
layout: article
title: July 2024
description: Notes for the month of July, 2024 (ongoing)
tag: microservices, docker
author: You
---

continuing last months article here, and in the MANY coming months lol

# running services using docker

## current deployment scenario

- we have servers running on 4000, 4001, 4002, 4003 and 4005
- now a simple solution is to take all this code and place it in a VM and run it there
- lets say comments service is being called too much, so we could create 2 more instances of comments and use a load balancer (load balancer basically randomises which comments server to send it to)
- and then wed have to use 2 more extra ports for these 2 new comments services
- and because we added these 2 new ports, we have to make changes in multiple projects, such as event bus and more
- and this way we're tightly coupling our code to the number of instances of comments
- and if you say we're going to have a separate VM for extra comments, we're still going to have to change multiple files and make changes
- hence we're going to be introd to docker and kube

## why docker

- we're going to create containers
- containers are like isolated computing envs
- it contains everything we need to run a single program
- so we're going to create separate containers for separate services
- if we need extra instances, we can just create another container with comments
- what do we need to run our services? npm and node right?
- now, if we need to run it somewhere we're assuming node and npm are installed no? and thats a big assumption
- and also it requires knowledge on which script to use to run the services
- so docker solves both these problems, its going to contain everything we need to run the service and will also know how to start it etc
- super easy when it comes to running services, not just for node js but for anything

## why kube

- tool for running a bunch of containers together
- when we run kube, we're supposed to give it a config file which tells it which all containers we'd like to run
- and then kube is going to handle comm and nw reqs btw all these containers
- so kube creates this thing called a cluster
- a cluster is a set of diff VMs, each VM is referred to as a node
- theyre all managed by something called a master and this master is a program thats going to manage everything inside of our cluster, all the programs and other aspects
- so lets say we have 3 nodes
  - 2 post
  - and 1 event bus
  - now when an event occurs, we'd have to tell the event bus how to reach the 2 post containers
  - when in fact, kube offers this "channel" , that lets us pass things to it and this channel will forward it to posts
  - so this way communication is super simple
- kube also makes copying containers and scaling super easy

## docker

- why should we use docker?
  - at some point we mustve installed some sw on our laptop
  - during the installation wizard, we may come across some error, we may have looked up the error and fixed that issue
  - and when the wizard continues, we may come across another error and then its the troubleshooting phase all over again
  - so, docker wants to make it super easy & straight forward to install and run sw on ANY device, laptop, servers etc
  - docker makes it really easy to install and run sw without worrying about setup or dependencies
- what is docker?
  - a bit more challenging to answer
  - when you read an article, or talk to someone, when they say "Oh i use docker", they most probably mean that they use the Docker ecosystem, and that Ecosystem consists of Docker Client, Server, Machine, Images, Hub, Compose
  - All these tools are pieces of sw that come together to form a platform to create containers
  - In essence, docker is a platform or ecosystem around creating and running containers
  - so to run redis, we ran `docker run -it redis` so when we ran this, something called docker cli reached out to docker hub and it downloaded a single file called an `Image`
  - an `Image` is a single file that contains all the dependencies and all the config req to run a ver specific program, for eg redis
  - a container is an essence of an image
  - a container is a program with its own isolated set of hw resources, its own memory, its own nw tech, its own hard drive
- installing docker
  - when we install docker, we're installing 2 key pieces of sw
  - called docker client (cli) and docker server (daemon)
  - cli - the tool that were issuing commands to
  - daemon - the tools thats responsible for creating images, running containers etc
- docker bts
  - we installed docker and ran `docker run hello-world`
  - here's all the things that happened bts
    - you gave the command to the cli
    - cli relayed this to the daemon
    - daemon checked if this image is available locally by looking into the `image cache`
    - since we just installed docker, we obv didnt have it, so the daemon reached out to docker hub, which is a repo of free images that we can download and run
    - so the daemon downloaded it from the hub and saved it locally in the image cache, where it can obtained from next time
    - then the daemon took that image, loaded it into the memory and created a container and ran it
- what is a container?
  - but before that,
    - what is NAMESPACING?
      - it is basically isolating your hard drive or any "resource" for that matter for processes or a group of processes
    - what are control groups? (cgroups)
      - they limit the amount of resources used per process
      - amount of memory, cpu, hard drive, and nw bandwidth
  - so in other words, a container is basically just a process and its necessary req resources
  - so the process talks to the kernel which in return talks to the resources that have been available for this process
  - what is the image -> container relation?
    - whenever we talk about images? we're talking specifically about file system (FS) snapshots and a start up command
    - what is a fs snapshot? its a copy-paste of some directories from the FS
    - so what happens when we run this image?
      - the kernel is going to allocate some part of the hard drive for the fs snapshot inside the image and store the snapshot data there
      - and then the startup command is run, and then the process is started and is isolated to just that container
- how is docker running on your device?
  - namespacing and cgroups are specific to linux
  - when we installed docker, we basically installed and are running a linux VM
  - inside this VM, we're creating all these containers
  - and the VM talks to the kernel and allocates resources etc

### creating and running an image

- `docker run <imagename>`
- we can also override the start up command by doing this
  - `docker run <imagename> command!`
  - after the imagename, we supply an alt command to be executed inside the container after it starts up
  - this is an override and the command that was there alongside the fs snapshot will not be run anymore
  - `docker run busybox echo hi there`
    - what does this mean?
    - we installed an image called busybox which contains .exe files called echo, ls, etc
    - and who knows what the startup command for busybox is? and we dont need to know it too as long as we know what command to run
    - so in this example, the busybox image has an exe called echo which repeats whatever you say to the docker cli
    - so when we run this command, we see hi there
  - `docker run busybox ls`
    - same understanding as above
    - busybox has an ls exe
    - so when we run it, we see the files that were copied to the container's hard drive by the fs snapshot
    - so whatever files you see, are all files that were copied from the fs snapshot and ARE NOT YOUR LOCAL FILES

## listing running containers

- `docker ps`
- this command will list all the diff running containers that are currently on our machine
- `docker ps --all`
- this command shows all the commands that have been called by you

## container lifecycle

- running `docker run` is equivalent to running `docker create` + `docker start`
- `docker create <image-name>` - creates a container
- `docker start <image-name>` - starts a container
- what happens when we create a container?
  - the fs snapshot from the image is taken and setup in the container's hardware
- what happens when we start a container?
  - we run the startup command that comes with the image
- now lets create and start a container

```bash
docker create hello-world
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768

docker start -a 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> output

%% what happens if we dont give the -a %%
docker start 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768

%% the -a attribute tells docker to watch for the output %%
```

## utils

### to clear your docker containers

- you can clear your stopped containers, build cache etc by running this command

```bash
docker system prune
```

- so now, if you want to run any image, it would first download the image from docker hub and then run it for us

### to get output logs

- now there maybe times when we forget to add the `-a` flag while using `docker start`
- and if it takes minutes/ hours to run, having to re run it again with the -a flag is just painful
- which is why we can use logging, such that whatever event is emitted from this container, its logged

```bash
docker start 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768

docker log 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> 4bf79a0bcd1a0703cf0d67c51bd31d58d375ca411c478782bf69e6962347a768
> hi there

```

### how to stop containers

- when we used to run docker run, we could stop the execution using cmd+c
- but if we use docker start and docker logs, how do we stop containers?
- `docker stop container-id` or `docker kill container-id`
- stop
  - when we use the stop command, bts, docker sends a signal to the container called SIGTERM
  - aka terminate signal
  - what this does is, it tells the container to terminate the signal in its own time and also gives the container a little time to perform some clean ups
  - if the container doesnt stop within 10s, docker automatically issues the kill command
- kill
  - when we use the kill command, bts, we send the SIGKILL command
  - aka kill signal
  - terminate and dont do anything else
- ideally wed like to stop containers

### execute an additional command in a container

- when using redis
- we usually run 2 commands, `redis-server` and `redis-cli`
- but if we run redis in a container, we cant access this redis server from outside (obvi)
- so then, in essence, we need to have another startup command along with the one that comes already with the image
- but how do we call it?
  `docker exec -it container-id command`
- exec - run another command
  -it - allwos us to provide input to the container
  command - the extra command you want to run
- an example:

```bash
%% shell 1 %%
docker run redis

%% shell 2 %%
docker ps
> CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS      NAMES
> d29c63477078   redis     "docker-entrypoint.s…"   5 seconds ago   Up 4 seconds   6379/tcp   trusting_poitras

docker exec -it d29c63477078 redis-cli
> 127.0.0.1:6379:> set mynumber 5
> 127.0.0.1:6379:> get mynumber
> "5"
```

- what happens if we dont give -it?
  - redis cli will be started but we wont have the ability to provide any inputs to this

#### purpose of -it flag

- in reality, the -it flag is a combination of a `-i` and a `-t` flag
- processes in linux have 3 channels lets say, STDIN, STDOUT and STDERR
- whatever ip you give, goes into the container using the STDIN channel, whatever the container spits out is shown to you via the STDOUT channel, and if any errors occur, theyre shown to you via the STDERR channel
- so when we type `-i` we're saying we want to attach this terminal session to the STDIN channel of the newly running process
- the `-t` flag simply formats it nicely for us to use, it does quite a bit under the hood, but simply for us to understand, it just makes the ips and ops pretty
- also provides autocomplete etc..

### how to get access to the terminal of your container?

- we will want to run commands inside our container without constantly wanting to type exec exec etc
- so if we want to access the terminal of a container, we can run this

```bash
docker exec -it container-id sh
> #
```

- and now we can run any linux or terminal commands like cd, ls, export etc
- and once youre in your shell and if you cant exit using ctrl+c, you can try ctrl+d

#### starting with a shell

- we can also run the `docker run -it busybox sh` command to start the busybox container, but just open the shell
- this way, we can poke around ourselves and execute whatever command we want

## creating docker images

- till now we've used images created by other devs, redis-cli, busybox, hello-world
- the process to create our own image is relatively straight forward, we just have to remember the syntax

1. create a dockerfile
   - this is a plain txt file with a few lines of config to define how our container must behave, what prgms it must contain and what it should do when it starts up
2. send it to docker client
3. client will send it to docker server
   - server is doing all the heavy lifting for us
   - its going to look inside the dockerfile and create an image that we can use
4. we have a usable image

- what is the flow to create a docker file?
  - specify a base image -> run some commands to install additional prgms -> specify a command to run on container setup
- here is a simple Dockerfile

```Dockerfile
# using the flow established earlier
# use an existing docker image as a base
FROM alpine

# download and install a dependency
RUN apk add --update redis

# tell the image what to do when it starts as a container
CMD ["redis-server"]
```

- then we cd into the file where we have this Dockerfile
- and we simply run `docker build .`
- and then in the end, we'll get one id of the image we created, then we can simply do `docker run image-id`

### breaking down the Dockerfile

- the Dockerfile contains a particular syntax
- the first word is an 'instruction'
- and whatever comes after it is an 'argument'
- so the instruction tells docker-server what to do
- the 'FROM' is used to specify what image we want to use as a base
- the 'RUN' instruction is used to execute some command while we are preparing our custom image
- the 'CMD' instruction is used to specify what should be executed when our image is used to start up a container
- FRO RUN and CMD are some of THE MOST IMP instructions, but there are many more that need to be known

#### what is a base image?

- writing a Dockerfile is similar to installing some browser on a comp with no OS
- what are the steps we'd do?
  - install an os -> open default browser -> download chrome dmg -> run dmg right?
- so what we did with `FROM alpine` was very similar
- that was like saying, install this OS (not exactly but just to paint a picture of what its like)
- otherwise it would be an empty image, no infra, no programs that we could use, nothing to help us install extra dependencies, nothing
- so having a base image is to give us a starting point of sorts that we can customise etc
- what is alpine?
  - simple, it contained the necessary programs and functions that we needed to create our custom image
  - very much like asking, why you chose win/ mac as your preferred os- because they provided you with what YOU needed
  -

#### notes

- Buildkit will hide away much of its progress which is something the legacy builder did not do. We will be discussing some messages and errors later in Section 4 that will be hidden by default. To see this output, you will want to pass the progress flag to the build command: `docker build --progress=plain .`
- Additionally, you can pass the no-cache flag to disable any caching: `docker build --no-cache --progress=plain .`
